<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Final Project: Neural Artistic Style Transfer</title>
</head>
<body>
<h1>Artistic Style Transfer Network</h1>
<h2>Submitted by Spencer Jenkins and Hugh Lau</h2>
The goal of this project was to use convolutional neural networks (CNN) for to implement Artistic Style
Transfer, involving taking the structural content of one image and applying the "style" content of
another image following the procedure outlined by
<a target="_blank" href="https://arxiv.org/pdf/1508.06576.pdf">Gatys, Ecker and Bethge</a>. This project demonstrates
the versatility of convolutional neural networks at collecting information at multiple abstract layers of an image,
and at producing aesthetically interesting outputs. <br>

<h3>Procedure</h3>
The project was implemented using PyTorch and executed on Google Colab.
A pre-trained version of the VGG-19 model, a highly effective convolutional model for visual recognition, was used along
with pretrained weights and normalization constants. Only the first 5 convolutional layers were taken.
Convolutional layers had a kernel size of 3x3, and a stride and padding of 1. Pooling was done with a
kernel and stride of 2. <br>In addition, MaxPool in VGG-19 was replaced with AvgPool in accordance with Gatys et al. for
better output quality.<br>
The overall model architecture used was: (Conv->ReLU->Conv->ReLU->Pool)*2->Conv<br>
The rest of the model was discarded for this project.<br><br>
Two loss functions were defined: Content Loss (CL) and Style Loss (SL). CL was the
mean squared difference between two images, while SL was the mean squared difference between
the Gram Matrices of two images. <br>
The <b>Gram Matrix</b> can be thought of as encoding first- and second-derivative
information about the photo.<br><br>
CL was calculated after convolutional layer 4 while SL was calculated after
all convolutional layers, following the paper.

Each image was first center-cropped and resized, then normalized according to parameters given by PyTorch alongside
the pretrained model weights. The goal is to take a copy of the content image, and optimize it minimizing CL and SL, each
weighed at a certain importance level. An L-BFGS optimizer was used.

<h3>Results</h3>

Check out some of the results we were able to create! Some of my favorites were from taking texture samples of
calligraphy. <br>
<img src="img/in/alsacien.jpg" height="30%" width="30%">
<img src="img/in/chinese_calligraphy.jpg" height="30%" width="30%"><br>
<img src="img/out/alsacien_chinese_calligraphy.jpg" height="30%" width="30%">
<img src="img/out/alsacien_chinese_calligraphy_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/alsacien.jpg" height="30%" width="30%">
<img src="img/in/arabic_calligraphy.jpg" height="30%" width="30%"><br>
<img src="img/out/alsacien_arabic_calligraphy.jpg" height="30%" width="30%">
<img src="img/out/alsacien_arabic_calligraphy_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/alsacien.jpg" height="30%" width="30%">
<img src="img/in/kiss.jpg" height="30%" width="30%"><br>
<img src="img/out/alsacien_kiss.jpg" height="30%" width="30%">
<img src="img/out/alsacien_kiss_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/cow.jpg" height="30%" width="30%">
<img src="img/in/arabic_calligraphy.jpg" height="30%" width="30%"><br>
<img src="img/out/cow_arabic_calligraphy.jpg" height="30%" width="30%">
<img src="img/out/cow_arabic_calligraphy_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/cow.jpg" height="30%" width="30%">
<img src="img/in/hieroglyphs.jpg" height="30%" width="30%"><br>
<img src="img/out/cow_hieroglyphs.jpg" height="30%" width="30%">
<img src="img/out/cow_hieroglyphs_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/cow.jpg" height="30%" width="30%">
<img src="img/in/kandinsky.jpg" height="30%" width="30%"><br>
<img src="img/out/cow_kandinsky.jpg" height="30%" width="30%">
<img src="img/out/cow_kandinsky_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/nyc.jpg" height="30%" width="30%">
<img src="img/in/starry-night.jpg" height="30%" width="30%"><br>
<img src="img/out/nyc_starry-night.jpg" height="30%" width="30%">
<img src="img/out/nyc_starry-night_loss.jpg" height="30%" width="30%"><br><br>
<img src="img/in/sd.jpg" height="30%" width="30%">
<img src="img/in/starry-night.jpg" height="30%" width="30%"><br>
<img src="img/out/sd_starry-night.jpg" height="30%" width="30%"><br><br>
<img src="img/in/wales.jpg" height="30%" width="30%">
<img src="img/in/hokusai.jpg" height="30%" width="30%"><br>
<img src="img/out/wales_hokusai.jpg" height="30%" width="30%">
<img src="img/out/wales_hokusai_loss.jpg" height="30%" width="30%"><br><br>
As a quick note, the texture copied over was not as detailed as expected. This could potentially be improved by
adding more layers to take loss at, adding more layers from GPT, or increasing the image dimensions.<br>
As one other note, you may notice random spikes in loss, which we currently cannot account for. It is our hypothesis
that this occurs when the gradient oversteps the region it intends to. <br>

No further bells and whistles were implemented. <br>
</body>
</html>
